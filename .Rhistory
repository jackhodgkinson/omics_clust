set.seed(seed)
if (parallel) {
if (.Platform$OS.type == "windows"){
# Set parallel cluster
clo <- makeCluster(n_cores)
clusterExport(clo, ls(envir = environment()), envir = environment())
# Loop over each element in data list
data_process <- function(data) {
# Create empty list for mclust and classification data frame
mclust <- vector("list", ncol(data))
classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
# Convert data frame to a list of columns
data2 <- as.list(data)
# Set up inner parallel cluster for columns
set.seed(seed)
cli <- makeCluster(n_cores)
clusterExport(cli, ls(envir = environment()), envir = environment())
clusterEvalQ(cli, library(mclust))
# Fit Mclust to each protein to obtain classification
class_res_in <- parLapply(cli, data2, function(i) {
colnames(i) <- NULL
mclust <- Mclust(i)
return(mclust$classification)  # Store clustered data column-wise
})
# Stop the cluster
stopCluster(cli)
# Combine classification results into a data frame
classification <- as.data.frame(do.call(cbind, class_res_in))
return(classification)
}
# Parallelize the processing of each dataset in the 'data' list (outer parallelization)
classification_results <- parLapply(clo, data, function(dataset) {
process_dataset(dataset)  # Process each dataset in parallel
})
# Stop the outer cluster after the work is done
stopCluster(clo)
# Name the results for each dataset
names(classification_results) <- paste0("Dataset", 1:length(data))
} else {
# Loop over each element in data list
data_process <- function(data) {
# Create empty list for mclust and classification data frame
mclust <- vector("list", ncol(data))
classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
# Convert data frame to a list of columns
data2 <- as.list(data)
# Set seed
set.seed(seed)
# Fit Mclust to each protein to obtain classification
class_res_in <- mclapply(data2, function(i) {
colnames(i) <- NULL
mclust <- Mclust(i)
return(mclust$classification)},
mc.cores = n_cores)
# Combine classification results into a data frame
classification <- as.data.frame(do.call(cbind, class_res_in))
return(classification)
}
# Parallelize the processing of each dataset in the 'data' list (outer parallelization)
classification_results <- mclapply(data, function(dataset) {process_dataset(dataset)},
mc.cores = n_cores)
# Name the results for each dataset
names(classification_results) <- paste0("Dataset", 1:length(data))
}
} else {
# Loop over each element in data list
data_process <- function(data) {
# Create empty list for mclust and classification data frame
mclust <- vector("list", ncol(data))
classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
# Convert data frame to a list of columns
data2 <- as.list(data)
# Fit Mclust to each protein to obtain classification
class_res_in <- lapply(data2, function(i) {
colnames(i) <- NULL
mclust <- Mclust(i)
return(mclust$classification)  # Store clustered data column-wise
})
# Combine classification results into a data frame
classification <- as.data.frame(do.call(cbind, class_res_in))
return(classification)
}
# Parallelize the processing of each dataset in the 'data' list (outer parallelization)
classification_results <-lapply(data, function(dataset) {
process_dataset(dataset)  # Process each dataset in parallel
})
# Name the results for each dataset
names(classification_results) <- paste0("Dataset", 1:length(data))
classification <- classification_results
}
}
return(classification)
}
# optimal_hclust.R
optimal_hclust <- function(dist_mat,
n_groups = NULL,
method,
index) {
# Figure out optimal number of groups
set.seed(seed)
# Try different linkage functions with different indices and see which is most accurate. Report this in thesis.
results <- data.frame(Method = character(),
Index = character(),
NGroups = integer(),
stringsAsFactors = FALSE)
all_index_list <- list()
for (m in method) {
for (i in index) {
try({
opt <- NbClust(diss = dist_mat, distance = NULL,
method = m, min.nc = 2, max.nc = 9,
index = i)
best_nc <- opt$Best.nc[[1]]
results <- rbind(results, data.frame(Method = m,
Index = i,
BestNC = best_nc))
index_df <- data.frame(NClust = as.numeric(names(opt$All.index)),
Value = as.numeric(opt$All.index),
Method = m,
Index = i)
all_index_list[[paste(m, i, sep = "_")]] <- index_df
}, silent = TRUE)
}
}
# Join with true number of groups
opt_results <- cbind(results, n_groups)
# Create data frame for plotting
all_index_df <- bind_rows(all_index_list)
# Plot graph
plot <- ggplot(all_index_df, aes(x = NClust, y = Value)) +
geom_line() +
facet_grid(Method ~ Index) +
labs(title = "NbClust Index Scores for Each Method and Index",
x = "Number of Groups",
y = "Index Value")
print(plot)
return(list(`Number of Clusters` = opt_results,
`Index Values` = all_index_df))
}
# optimal_hclust.R
optimal_hclust <- function(dist_mat,
n_groups = NULL,
method,
index) {
# Figure out optimal number of groups
set.seed(seed)
# Try different linkage functions with different indices and see which is most accurate. Report this in thesis.
results <- data.frame(Method = character(),
Index = character(),
NGroups = integer(),
stringsAsFactors = FALSE)
all_index_list <- list()
for (m in method) {
for (i in index) {
try({
opt <- NbClust(diss = dist_mat, distance = NULL,
method = m, min.nc = 2, max.nc = 9,
index = i)
best_nc <- opt$Best.nc[[1]]
results <- rbind(results, data.frame(Method = m,
Index = i,
BestNC = best_nc))
index_df <- data.frame(NClust = as.numeric(names(opt$All.index)),
Value = as.numeric(opt$All.index),
Method = m,
Index = i)
all_index_list[[paste(m, i, sep = "_")]] <- index_df
}, silent = TRUE)
}
}
# Join with true number of groups
opt_results <- cbind(results, n_groups)
# Create data frame for plotting
all_index_df <- bind_rows(all_index_list)
return(list(`Number of Clusters` = opt_results,
`Index Values` = all_index_df))
}
## clusterofclusters.R
clusterofclusters <- function(moc,                                              # Matrix of Clusters - N X C data matrix, where C is the total number of clusters.
k = 2:9,                                          # Number of clusters. Default is to loop through k = 2 to k = 9.
N = 1000,                                         # Number of iterations of Consensus Clustering step.
max.iter = 1000,                                  # Maximum number of iterations for k-means clustering
pItem = 0.8,                                      # Proportion of items sampled at each iteration.
hclustMethod = "average",                         # Agglomeration method to be used by the hclust function to perform hierarchical clustering on the consensus matrix. Can be "single","complete", "average", etc. For more details please see ?stats::hclust.
choiceKmethod = "silhouette",                     # Method used to choose the number of clusters if K is NULL, can be either "AUC" (area under the curve, work in progress) or "silhouette". Default is "silhouette".
ccClMethod = "kmeans",                            # Clustering method to be used by the Consensus Clustering algorithm (CC). Can be either "kmeans" for k-means clustering or "hclust" for hiearchical clustering. Default is "kmeans".
ccDistHC = "euclidean",                           # Distance to be used by the hiearchical clustering algorithm inside CC. Can be "pearson" (for 1 - Pearson correlation), "spearman" (for 1- Spearman correlation), or any of the distances provided in stats::dist() (i.e. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"). Default is "euclidean".
savePNG = FALSE,                                  # Boolean. Save plots as PNG files. Default is FALSE.
fileName = "coca",                                # Boolean. If savePNG is TRUE, this is the string containing (the first part of) the name of the output files. Can be used to specify the folder path too. Default is "coca". The ".png" extension is automatically added to this string.
verbose = FALSE,                                  # Boolean.
widestGap = FALSE,                                # Boolean. If TRUE, compute also widest gap index to choose best number of clusters. Default is FALSE.
dunns = FALSE,                                    # Boolean. If TRUE, compute also Dunn's index to choose best number of clusters. Default is FALSE.
dunn2s = FALSE,                                   # Boolean. If TRUE, compute also alternative Dunn's index to choose best number of clusters. Default is FALSE.
returnAllMatrices = FALSE,                        # Boolean. If TRUE, return consensus matrices for all considered values of K. Default is FALSE.
random_seed = NULL,                               # Set random seed for reproducibility. Default is NULL
parallel = TRUE                                   # Use parallel processsing. Default is TRUE.
)
{   # Install relevant packages
library(parallel)
# Set random seed
if(!is.null(random_seed)){
set.seed(random_seed)
}
# Intialise output list
output <- list()
n <- dim(moc)[1]
if (is.null(k)) {
stop("Parameter 'k' is required. Please specify a value or range for 'k'.")
}
# Run COCA with parallel processing
if (parallel == TRUE) {
if (length(k) != 1 & choiceKmethod == "silhouette") {
# Create a cluster with available cores
cl <- makeCluster(min(length(k), detectCores() - 1))
# Export necessary variables and functions to the cluster
clusterExport(cl, ls(envir = environment()), envir = environment())
# Run the parallelized operation using parLapply
results <- parLapply(cl, k, function(i) {
### Step 1: Compute consensus matrix for each k
cm <- coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
dist = ccDistHC, maxIterKM = max.iter)
### Step 2. Use hierarchical clustering on the consensus matrix
dist_mat <- stats::as.dist(1 - cm)
hc <- stats::hclust(dist_mat, method = hclustMethod)
labels <- stats::cutree(hc, i)
# Return results per k
return(list(k = i, consensusMatrix = cm, clusterLabels = labels))
})
stopCluster(cl)
# Combine results for silhouette evaluation
consensusArray <- array(NA, dim = c(nrow(moc), nrow(moc), length(k)))
clLabels <- matrix(NA, nrow = length(k), ncol = nrow(moc))
for (j in seq_along(k)) {
consensusArray[, , j] <- results[[j]]$consensusMatrix
clLabels[j, ] <- results[[j]]$clusterLabels
}
consensusMatrix <- consensusArray
K <- coca::maximiseSilhouette(consensusMatrix, clLabels, max(k), savePNG,
fileName, widestGap = widestGap, dunns = dunns,
dunn2s = dunn2s)$K
# return(K)
} else if (length(k) != 1 & choiceKmethod == "AUC") {
# Create a cluster with available cores
cl <- makeCluster(min(length(k), detectCores() - 1))
# Export necessary variables and functions to the cluster
clusterExport(cl, ls(envir = environment()), envir = environment())
# Run the parallelized operation using parLapply
results <- parLapply(cl, k, function(i) {
### Step 1. Compute the consensus matrix ###
cm <- coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
dist = ccDistHC, maxIterKM = max.iter)
### Step 2. Compute area under the curve ###
auc <- computeAUC(cm)
return(list(k = i, consensusMatrix = cm, auc = auc))
})
stopCluster(cl)
# Collect AUC values and consensus matrices
areaUnderTheCurve <- sapply(results, function(res) res$auc)
consensusArray <- array(NA, dim = c(nrow(moc), nrow(moc), length(k)))
for (j in seq_along(k)) {
consensusArray[, , j] <- results[[j]]$consensusMatrix
}
# Step 3: Pick K using AUC
K <- coca::chooseKusingAUC(areaUnderTheCurve, savePNG, fileName)$K
consensusMatrix <- consensusArray
#  return(K)
} else if (length(k) != 1) {
stop("Method to choose number of clusters has not been recognised.
Please make sure that it is either `silhouette` or `AUC`.")
}
else if (length(k) == 1) {
consensusMatrix <- NULL
K <- k
}
}
# Run COCA without parallel processing
else {
if (length(k) != 1 & choiceKmethod == "silhouette") {
consensusMatrix <- array(NA, c(n, n, max(k) - 1))
clLabels <- array(NA, c(max(k) - 1, n))
for (i in seq_len(max(k) - 1) + 1) {
### Step 1. Compute the consensus matrix
consensusMatrix[, ,i - 1] <-
coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
dist = ccDistHC, maxIterKM = max.iter)
### Step 2. Use hierarchical clustering on the consensus matrix
distances <- stats::as.dist(1 - consensusMatrix[, , i - 1])
hClustering <- stats::hclust(distances, method = hclustMethod)
clLabels[i - 1, ] <- stats::cutree(hClustering, i)
}
K <- coca::maximiseSilhouette(consensusMatrix, clLabels, max(k), savePNG,
fileName, widestGap = widestGap, dunns = dunns,
dunn2s = dunn2s)$K
# return(K)
} else if (length(k) != 1 & choiceKmethod == "AUC") { # I think this needs to be removed and changed to dunn!
consensusMatrix <- array(NA, c(n, n, max(k) - 1))
areaUnderTheCurve <- rep(NA, max(k) - 1)
for (i in seq_len(max(k) - 1) + 1) {
### Step 1. Compute the consensus matrix ###
consensusMatrix[, ,i - 1] <-
coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
dist = ccDistHC, maxIterKM = max.iter)
### Step 2. Compute area under the curve ###
areaUnderTheCurve[i - 1] <- computeAUC(consensusMatrix[, , i - 1])
}
K <- coca::chooseKusingAUC(areaUnderTheCurve, savePNG, fileName)$K
#  return(K)
#
} else if (length(k) != 1) {
stop("Method to choose number of clusters has not been recognised.
Please make sure that it is either `silhouette` or `AUC`.")
} else if (length(k) == 1) {
consensusMatrix <- NULL
K <- k
}
}
if (verbose)
print(paste("K =", k, sep = " "))
### Step 1. Compute the consensus matrix ###
if (!is.null(consensusMatrix)) {
output$consensusMatrix <- consensusMatrix[, , K - 1]
output$consensusMatrix <- matrix(as.integer(output$consensusMatrix),
nrow = nrow(output$consensusMatrix),
ncol = ncol(output$consensusMatrix))
} else {
output$consensusMatrix <- coca::consensusCluster(moc, K, B = N, pItem,
clMethod = ccClMethod,
dist = ccDistHC,
maxIterKM = max.iter)
output$consensusMatrix <- matrix(as.integer(output$consensusMatrix),
nrow = nrow(output$consensusMatrix),
ncol = ncol(output$consensusMatrix))
}
### Step 2. Use hierarchical clustering on the consensus matrix ###
d <- stats::as.dist(1 - output$consensusMatrix)
hC <- stats::hclust(d, method = hclustMethod)
output$clusterLabels <- stats::cutree(hC, K)
output$K <- K
if (returnAllMatrices)
output$consensusMatrices <- consensusMatrix
return(output)
}
# multicoca.R
multicoca <- function(moc_list,                                         # List of Matrix of Clusters
k = 2:9,                                          # Number of clusters. Default is to loop through k = 1 to k = 9.
N = 1000,                                         # Number of iterations of Consensus Clustering step.
max.iter = 1000,                                  # Maximum number of iterations for k-means clustering
pItem = 0.8,                                      # Proportion of items sampled at each iteration.
hclustMethod = "average",                         # Agglomeration method to be used by the hclust function to perform hierarchical clustering on the consensus matrix. Can be "single","complete", "average", etc. For more details please see ?stats::hclust.
choiceKmethod = "silhouette",                     # Method used to choose the number of clusters if K is NULL, can be either "AUC" (area under the curve, work in progress) or "silhouette". Default is "silhouette".
ccClMethod = "kmeans",                            # Clustering method to be used by the Consensus Clustering algorithm (CC). Can be either "kmeans" for k-means clustering or "hclust" for hiearchical clustering. Default is "kmeans".
ccDistHC = "euclidean",                           # Distance to be used by the hiearchical clustering algorithm inside CC. Can be "pearson" (for 1 - Pearson correlation), "spearman" (for 1- Spearman correlation), or any of the distances provided in stats::dist() (i.e. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"). Default is "euclidean".
savePNG = FALSE,                                  # Boolean. Save plots as PNG files. Default is FALSE.
fileName = "coca",                                # Boolean. If savePNG is TRUE, this is the string containing (the first part of) the name of the output files. Can be used to specify the folder path too. Default is "coca". The ".png" extension is automatically added to this string.
verbose = FALSE,                                  # Boolean.
widestGap = FALSE,                                # Boolean. If TRUE, compute also widest gap index to choose best number of clusters. Default is FALSE.
dunns = FALSE,                                    # Boolean. If TRUE, compute also Dunn's index to choose best number of clusters. Default is FALSE.
dunn2s = FALSE,                                   # Boolean. If TRUE, compute also alternative Dunn's index to choose best number of clusters. Default is FALSE.
returnAllMatrices = FALSE,                        # Boolean. If TRUE, return consensus matrices for all considered values of K. Default is FALSE.
random_seed = NULL,                               # Set random seed for reproducibility. Default is NULL
parallel = FALSE                                   # Use parallel processing. Default is TRUE.
)
{
# Load necessary libraries
library(coca)
library(parallel)
# Source function
source("clusterofclusters.R")
# Set random seed
if(!is.null(random_seed)){
set.seed(random_seed)
}
# Create an empty list to store the results
results <- list()
# Run MultiCOCA with parallel processing
if (parallel == TRUE) {
# Create a cluster with available cores
cl <- makeCluster(detectCores() - 1)
# Export necessary variables and functions to the cluster
clusterExport(cl, varlist = c("clusterofclusters"))
# Run the parallelized operation using parLapply
results <- parLapply(cl, 1:length(moc_list), function(i) {
result <- clusterofclusters(moc_list[[i]],
k = k,
N = N,
max.iter = max.iter,
pItem = pItem,
hclustMethod = hclustMethod,
choiceKmethod = choiceKmethod,
ccClMethod = ccClMethod,
ccDistHC = ccDistHC,
savePNG = savePNG,
fileName = paste0(fileName, "_Group", i),
verbose = verbose,
widestGap = widestGap,
dunns = dunns,
dunn2s = dunn2s,
returnAllMatrices = returnAllMatrices,
random_seed = random_seed,
parallel = TRUE)
# Name the results for each group
return(result)
})
# Stop the cluster after computation
stopCluster(cl)
# Name each group
names(results) <- paste0("Group", 1:length(results))
}
# Run MultiCOCA without parallel processing
else {
for (i in 1:length(moc_list)) {
results[[i]] <- clusterofclusters(moc_list[[i]],
k = k,
N = N,
max.iter = max.iter,
pItem = pItem,
hclustMethod = hclustMethod,
choiceKmethod = choiceKmethod,
ccClMethod = ccClMethod,
ccDistHC = ccDistHC,
savePNG = savePNG,
fileName = paste0(fileName, "_Group", i),
verbose = verbose,
widestGap = widestGap,
dunns = dunns,
dunn2s = dunn2s,
returnAllMatrices = returnAllMatrices,
random_seed = random_seed,
parallel = FALSE)
names(results)[[i]] <- paste0("Group", i)
}
}
# Return the results list
return(results)
}
# groupARI.R
groupARI <- function(output,
true_clusters){
group_names <- names(results)
ari_values <- data.frame(Group = character(),
ARI = numeric(),
stringsAsFactors = FALSE
)
for (group in group_names){
ari_values <- rbind(ari_values, data.frame(Group = group,
ARI = adjustedRandIndex(
true_clusters[[paste0(tolower(group),"_clusterid")]],
output[[group]]$clusterLabels)
))
}
return(ari_values)
}
N_col <- 10
n_groups <- 3
params <- list(
cluster1 = list(mean = rnorm(N_col, mean = -2, sd = 0.1), cov = cov(matrix(rnorm(N_col*N_col, mean = 0.3, sd = 0.3), nrow = N_col, ncol = N_col))),
cluster2 = list(mean = rnorm(N_col, mean = 0, sd = 0.2), cov = cov(matrix(rnorm(N_col*N_col, mean = 0.6, sd = 0.25), nrow = N_col, ncol = N_col))),
cluster3 = list(mean = rnorm(N_col, mean = 3, sd = 0.2), cov = cov(matrix(rnorm(N_col*N_col, mean = -0.4, sd = 0.2), nrow = N_col, ncol = N_col)))
)
data <- simulateGMM(3, n_groups, params, n_indiv = 419, n_col = N_col,
random_seed = seed,
equal_clust = FALSE, equal_groups = FALSE)
true_clusters <- data[[2]]
if (n_groups > 1) {
true_groups <- data[[3]]
}
data <- data[[1]]
# Classify data using a GMM
classification <- GMMclassifier(data)
# Construct similarity matrix
sim_matrix <- matrix(0, nrow = ncol(classification), ncol = ncol(classification),
dimnames = list(colnames(classification), colnames(classification)))
set.seed(seed)
for (i in 1:ncol(classification)){
for(j in 1:ncol(classification)){
sim_matrix[i,j] <- adjustedRandIndex(classification[[i]], classification[[j]]) # we have called this M (capital)
}
}
# Convert to dissimilarity matrix
dissim_matrix <- 1 - sim_matrix
dist_mat <- as.dist(dissim_matrix)
# Pheatmap
pheatmap::pheatmap(sim_matrix)
# Specify methods and indices to test
methods <- list("single","complete","average","ward.D2","ward.D","mcquitty","median","centroid")
indices <- list("cindex","silhouette","dunn","mcclain")
opt_hclust <- optimal_hclust(dist_mat,
n_groups,
method = methods,
index = indices)
# Choose best index and linkage function
opt_hclust$`Number of Clusters`
# Plot results
indices <- opt_hclust$`Index Values`
ggplot(data = indices, x = )
# Choose best index and linkage function
opt_hclust$`Number of Clusters`
# Plot results
indices <- opt_hclust$`Index Values`
ggplot(data = indices, aes(x = NClust, y = Value)) +
geom_line() +
facet_wrap(Method ~ Index)
indices <- opt_hclust$`Index Values`
ggplot(data = indices, aes(x = NClust, y = Value)) +
geom_line() +
facet_grid(Method ~ Index) +  # Facet by both Method and Index
labs(
title = "Clustering Indices by Number of Clusters",
x = "Number of Clusters",
y = "Index Value"
) +
theme_minimal()
