---
title: "MultiCOCA Test File"
author: "Jack Hodgkinson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r seed}
seed <- 4881
set.seed(seed)
```

```{r setup2, echo=FALSE}
# Load packages
library(mclust)
library(NbClust)
library(tidyverse)
library(shiny)
```

```{r numCores, echo = FALSE}
# numCores.R
numCores <- function(fallback = 1) {
  # Try multiple known environment variables used by different schedulers
  env_vars <- c(
    "SLURM_CPUS_PER_TASK",     # Slurm
    "PBS_NUM_PPN",             # PBS
    "NSLOTS",                  # SGE
    "OMP_NUM_THREADS",         # OpenMP or general hint
    "NUM_THREADS",             # Generic cloud var
    "NUMBER_OF_PROCESSORS"     # Windows-specific sometimes
  )
  
  # Try each environment variable
  for (var in env_vars) {
    val <- Sys.getenv(var, unset = NA)
    if (!is.na(val) && nzchar(val)) {
      cores <- suppressWarnings(as.integer(val))
      if (!is.na(cores) && cores > 0) return(cores)
    }
  }
  
  # Fall back to local detection
  cores <- parallel::detectCores(logical = FALSE)
  if (is.na(cores) || cores < 1) return(fallback)
  return(cores)
}
```

```{r simulateGMM, echo=FALSE}
simulateGMM <- function(n_clust,                                                 # Number of clusters
                        n_groups,                                                # Number of groups of data, 1 by default
                        cluster_params,                                          # List of distribution parameters per cluster
                        n_indiv,                                                 # Number of individuals  
                        n_col,                                                   # Number of columns in simulated data
                        random_seed,                                             # Input random seed for reproducibility
                        cluster_labels = NULL,                                   # Input cluster labels, NULL by default.
                        group_labels = NULL,                                     # Input group labels, NA by default
                        equal_clust = TRUE,                                      # If generating cluster labels, ensure each cluster has approx equal individuals
                        equal_groups = TRUE,                                     # If n_groups > 1, ensure each group contains approx equal number of cols
                        parallel = FALSE                                          # Use parallel processsing. Default is TRUE
                        ){
  
  # Load relevant packages
  library(MASS)
  library(parallel)
  
  # Ensure numeric inputs
  n_clust <- as.numeric(n_clust)
  n_groups <- as.numeric(n_groups)
  n_indiv <- as.numeric(n_indiv)
  n_col <- as.numeric(n_col)
  random_seed <- as.numeric(random_seed)

  # Set seed
  set.seed(random_seed)
  
  # Validate parameters
  if (length(cluster_params) != n_clust) {
    stop(paste0("The number of clusters (", n_clust, ") does not match the number of elements in cluster_params (", length(cluster_params), ")."))
  }
  invisible(lapply(seq_len(n_clust), function(k) {
    clust_name <- paste0("cluster", k)
    params <- cluster_params[[clust_name]]
    if (is.null(params)) stop(paste("Missing parameters for", clust_name))
    if (is.null(params$mean)) stop(paste("Missing 'mean' for", clust_name))
    if (length(params$mean) != n_col) stop(paste("Length of 'mean' must match n_col for", clust_name))
    if (is.null(params$cov) && is.null(params$sd)) {
      stop(paste("Must provide either 'cov' or 'sd' for", clust_name))
    }
    if (!is.null(params$sd) && length(params$sd) != n_col) {
      stop(paste("Length of 'sd' must match n_col for", clust_name))
    }
    if (!is.null(params$cov) && (!is.matrix(params$cov) || any(dim(params$cov) != n_col))) {
      stop(paste("Covariance matrix for", clust_name, "must be", n_col, "x", n_col))
    }
    if (!is.null(params$sd) && !is.null(params$cov)) {
      warning(paste("Both 'sd' and 'cov' provided for", clust_name, "- using 'cov'"))
    }
  }))
  
  # Generate cluster labels
  indiv_clust <- if (is.null(cluster_labels)){
    if (!equal_clust){
      sample(seq_len(n_clust), size = n_indiv, replace = TRUE, 
             prob = {p <- runif(n_clust); p / sum(p)})
    } else {
      sample(seq_len(n_clust), size = n_indiv, replace = TRUE)  
    } 
  } else cluster_labels
  
  # Data simulation function
  sim_clust_data <- function (k) {
    idx <- which(indiv_clust == k)
    if (length(idx) == 0) return(NULL)
    param <- cluster_params[[paste0("cluster",k)]]
    mu <- param$mean
    Sigma <- if (!is.null(param$cov)) param$cov else diag(param$sd^2)
    data <- mvrnorm(n = length(idx), mu = mu, Sigma = Sigma)
    list(index = idx, data = data)
  }
  
  # Generate data 
  if (parallel && n_indiv > 1000 && n_clust > 3) {
    if (.Platform$OS.type != "windows"){
      cluster_results <- mclapply(seq_len(n_clust), sim_clust_data, 
                                  mc.cores = detectCores() - 1)
    }
    else {
      cl <- makeCluster(detectCores() - 1)
      clusterExport(cl, ls(envir = environment()), envir = environment())
      cluster_results <- parLapply(cl, seq_len(n_clust), sim_clust_data)
      stopCluster(cl)
    }
  } else { 
      cluster_results <- lapply(seq_len(n_clust), sim_clust_data)
    }
      
    # Create data matrix 
    sim_data <- matrix(NA, nrow = n_indiv, ncol = n_col)
    invisible(lapply(cluster_results, function(res) {
      if (!is.null(res)) sim_data[res$index, ] <<- res$data
    }))
    
    # Assign empty group
    group <- NULL
    
    # Permute data if different clustering structures required
    if (n_groups > 1) {
      group <- if (!is.null(group_labels)) {
        group_labels
      } else if (equal_groups == FALSE & !is.null(group)){
        repeat {
          p <- runif(n_groups)
          p <- p / sum(p)  # normalize to sum to 1
          group <- sample(seq_len(n_groups), n_col, replace = TRUE, prob = p)
          if (length(unique(group)) == n_groups) break
        }
        group 
      } else {
        sample(seq_len(n_groups), n_col, replace = TRUE)
      }
      
      # Run in parallel if needed
      if (parallel && n_indiv > 1000 && n_groups > 2) {
        if (.Platform$OS.type != "windows") {
          cl <- mclapply(seq_len(n_groups - 1), function(g) {
            permute_cols <- which(group == g)
            order <- sample(n_indiv)
            sim_data[, permute_cols] <<- sim_data[order, permute_cols]
            out <- indiv_clust[order]
            names(out) <- paste0("group", g, "_clusterid")
            out
          }, mc.cores = detectCores() - 1)
        } else {
          cl <- makeCluster(n_cores)
          clusterExport(cl, c("sim_data", "group", "n_indiv", "indiv_clust"), envir = environment())
          permuted_list <- parLapply(cl, seq_len(n_groups - 1), function(g) {
            permute_cols <- which(group == g)
            order <- sample(n_indiv)
            sim_data[, permute_cols] <<- sim_data[order, permute_cols]
            out <- indiv_clust[order]
            names(out) <- paste0("group", g, "_clusterid")
            out
          })
          stopCluster(cl)
        }
    } else {
      # Serial column permutations for groups
      permuted_list <- lapply(seq_len(n_groups - 1), function(g) {
        permute_cols <- which(group == g)
        order <- sample(n_indiv)
        sim_data[, permute_cols] <<- sim_data[order, permute_cols]
        out <- indiv_clust[order]
        names(out) <- paste0("group", g, "_clusterid")
        out
      })
    }
      
    names(permuted_list) <- paste0("group", seq_len(n_groups-1), "_clusterid")
    permuted_list[[paste0("group", n_groups, "_clusterid")]] <- indiv_clust
    group_clusterID <- as.data.frame(permuted_list)
    
    sim_data <- as.data.frame(sim_data)
    
    return(list("Simulated Data" = sim_data,
                "Cluster ID per individual per group" = group_clusterID,
                "Group ID"= group))
  } else {
      sim_data <- as.data.frame(sim_data)
      return(list("Simulated Data" = sim_data,
                  "Cluster ID" = indiv_clust))
  }
}
```

```{r GMMclassifier, echo=FALSE}
# GMMclassifier.R
# Function
GMMclassifier <- function(data,
                          parallel = TRUE) {
  
  # Detect number of cores
  n_cores <- numCores()
  
  # Cluster each column if single dataset
  if (class(data) != "list") {
    
    # Create empty list for mclust and classification data frame 
    mclust <- vector("list", ncol(data))
    classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
    
    # Convert data frame to a list of columns 
    data2 <- as.list(data)
    
    # Set seed
    set.seed(seed)
    
    if (parallel) {
    
      if (.Platform$OS.type == "windows"){
      
        # Set up parallel cluster
        cl <- makeCluster(n_cores)
        clusterExport(cl, ls(envir = environment()), envir = environment())
        
        # Fit Mclust to each protein to obtain classification
        classification_results <- parLapply(cl, data2, function(i) {
          colnames(i) <- NULL
          return(Mclust(i)$classification)
        })
        
        # Combine into a classification data frame
        classification <- as.data.frame(do.call(cbind, classification_results))
      
        # End parallel cluster
        stopCluster(cl) 
        
    } else {
        
        # Fit Mclust to each protein to obtain classification
        classification_results <- mclapply(data2, function(i) {
          colnames(i) <- NULL
          return(mclust::Mclust(i)$classification)
        },
        mc.cores = n_cores)
        
        # Combine into a classification data frame
        classification <- as.data.frame(do.call(cbind, classification_results))
        
    }
    } else {
          
          # Fit Mclust to each protein to obtain classification
          classification_results <- lapply(data2, function(i) {
            colnames(i) <- NULL
            return(Mclust(i)$classification)
          })
          
          # Combine into a classification data frame
          classification <- as.data.frame(do.call(cbind, classification_results))
        }
      }
    
    # Cluster each column if a list of datasets
    else {
    
    # Initialize lists to store Mclust results and classification matrices for each data frame
    mclust_results <- list()  
    classification_results <- list() 
    
    # Set seed 
    set.seed(seed)
    
    if (parallel) {
    
      if (.Platform$OS.type == "windows"){
      
        # Set parallel cluster
        clo <- makeCluster(n_cores)
        clusterExport(clo, ls(envir = environment()), envir = environment())
        
        # Loop over each element in data list
        data_process <- function(data) {
          
          # Create empty list for mclust and classification data frame 
          mclust <- vector("list", ncol(data))
          classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
          
          # Convert data frame to a list of columns 
          data2 <- as.list(data)
          
          # Set up inner parallel cluster for columns
          set.seed(seed)
          cli <- makeCluster(n_cores)
          clusterExport(cli, ls(envir = environment()), envir = environment())
          clusterEvalQ(cli, library(mclust))
          
          # Fit Mclust to each protein to obtain classification 
          class_res_in <- parLapply(cli, data2, function(i) {
            colnames(i) <- NULL
            mclust <- Mclust(i)
            return(mclust$classification)  # Store clustered data column-wise
          })
          
          # Stop the cluster
          stopCluster(cli)
          
          # Combine classification results into a data frame
          classification <- as.data.frame(do.call(cbind, class_res_in))
          return(classification)
        }
        
        # Parallelize the processing of each dataset in the 'data' list (outer parallelization)
        classification_results <- parLapply(clo, data, function(dataset) {
          process_dataset(dataset)  # Process each dataset in parallel
        })
        
        # Stop the outer cluster after the work is done
        stopCluster(clo)
        
        # Name the results for each dataset
        names(classification_results) <- paste0("Dataset", 1:length(data))
        
    } else {
      
      # Loop over each element in data list
      data_process <- function(data) {
        
        # Create empty list for mclust and classification data frame 
        mclust <- vector("list", ncol(data))
        classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
        
        # Convert data frame to a list of columns 
        data2 <- as.list(data)
        
        # Set seed
        set.seed(seed)
        
        # Fit Mclust to each protein to obtain classification 
        class_res_in <- mclapply(data2, function(i) {
          colnames(i) <- NULL
          mclust <- Mclust(i)
          return(mclust$classification)},
          mc.cores = n_cores)
        
        # Combine classification results into a data frame
        classification <- as.data.frame(do.call(cbind, class_res_in))
        return(classification)
      }
      
      # Parallelize the processing of each dataset in the 'data' list (outer parallelization)
      classification_results <- mclapply(data, function(dataset) {process_dataset(dataset)},
                                         mc.cores = n_cores)
      
      # Name the results for each dataset
      names(classification_results) <- paste0("Dataset", 1:length(data))
        
    }
      
    } else {
    
      # Loop over each element in data list
      data_process <- function(data) {
        
        # Create empty list for mclust and classification data frame 
        mclust <- vector("list", ncol(data))
        classification <- as.data.frame(matrix(NA, nrow = nrow(data), ncol = ncol(data)))
        
        # Convert data frame to a list of columns 
        data2 <- as.list(data)
        
        # Fit Mclust to each protein to obtain classification 
        class_res_in <- lapply(data2, function(i) {
          colnames(i) <- NULL
          mclust <- Mclust(i)
          return(mclust$classification)  # Store clustered data column-wise
        })
        
        # Combine classification results into a data frame
        classification <- as.data.frame(do.call(cbind, class_res_in))
        return(classification)
      }
      
      # Parallelize the processing of each dataset in the 'data' list (outer parallelization)
      classification_results <-lapply(data, function(dataset) {
        process_dataset(dataset)  # Process each dataset in parallel
      })
      
      # Name the results for each dataset
      names(classification_results) <- paste0("Dataset", 1:length(data))
      classification <- classification_results
      
    }
    }
  
  return(classification)
}
```

```{r optimal_hclust, echo=FALSE}
# optimal_hclust.R 
optimal_hclust <- function(dist_mat,
                           n_groups = NULL,
                           method, 
                           index) { 
  
  # Figure out optimal number of groups 
  set.seed(seed)
  
  # Try different linkage functions with different indices and see which is most accurate. Report this in thesis.
  results <- data.frame(Method = character(), 
                        Index = character(), 
                        NGroups = integer(), 
                        stringsAsFactors = FALSE)
  
  all_index_list <- list()
  
  for (m in method) {
    for (i in index) {
      try({
        opt <- NbClust(diss = dist_mat, distance = NULL,
                       method = m, min.nc = 2, max.nc = 9,
                       index = i)
        best_nc <- opt$Best.nc[[1]]
        results <- rbind(results, data.frame(Method = m,
                                             Index = i,
                                             BestNC = best_nc))
        index_df <- data.frame(NClust = as.numeric(names(opt$All.index)),
                               Value = as.numeric(opt$All.index),
                               Method = m,
                               Index = i)
        
        all_index_list[[paste(m, i, sep = "_")]] <- index_df
      }, silent = TRUE)
    }
  }
  
  # Join with true number of groups 
  opt_results <- cbind(results, n_groups)
  
  # Create data frame for plotting
  all_index_df <- bind_rows(all_index_list)
  
  # Plot graph 
  plot <- ggplot(all_index_df, aes(x = NClust, y = Value)) + 
    geom_line() + 
    facet_grid(Method ~ Index) + 
    labs(title = "NbClust Index Scores for Each Method and Index",
         x = "Number of Groups",
         y = "Index Value")
  
  print(plot)
  
  return(list(`Number of Clusters` = opt_results, 
              `Index Values` = all_index_df))
}
```

```{r clusterofclusters, echo = FALSE}
## clusterofclusters.R
clusterofclusters <- function(moc,                                              # Matrix of Clusters - N X C data matrix, where C is the total number of clusters.
                              k = 2:9,                                          # Number of clusters. Default is to loop through k = 2 to k = 9.
                              N = 1000,                                         # Number of iterations of Consensus Clustering step.
                              max.iter = 1000,                                  # Maximum number of iterations for k-means clustering
                              pItem = 0.8,                                      # Proportion of items sampled at each iteration. 
                              hclustMethod = "average",                         # Agglomeration method to be used by the hclust function to perform hierarchical clustering on the consensus matrix. Can be "single","complete", "average", etc. For more details please see ?stats::hclust.
                              choiceKmethod = "silhouette",                     # Method used to choose the number of clusters if K is NULL, can be either "AUC" (area under the curve, work in progress) or "silhouette". Default is "silhouette".
                              ccClMethod = "kmeans",                            # Clustering method to be used by the Consensus Clustering algorithm (CC). Can be either "kmeans" for k-means clustering or "hclust" for hiearchical clustering. Default is "kmeans".
                              ccDistHC = "euclidean",                           # Distance to be used by the hiearchical clustering algorithm inside CC. Can be "pearson" (for 1 - Pearson correlation), "spearman" (for 1- Spearman correlation), or any of the distances provided in stats::dist() (i.e. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"). Default is "euclidean".
                              savePNG = FALSE,                                  # Boolean. Save plots as PNG files. Default is FALSE.
                              fileName = "coca",                                # Boolean. If savePNG is TRUE, this is the string containing (the first part of) the name of the output files. Can be used to specify the folder path too. Default is "coca". The ".png" extension is automatically added to this string.
                              verbose = FALSE,                                  # Boolean. 
                              widestGap = FALSE,                                # Boolean. If TRUE, compute also widest gap index to choose best number of clusters. Default is FALSE.
                              dunns = FALSE,                                    # Boolean. If TRUE, compute also Dunn's index to choose best number of clusters. Default is FALSE.
                              dunn2s = FALSE,                                   # Boolean. If TRUE, compute also alternative Dunn's index to choose best number of clusters. Default is FALSE.
                              returnAllMatrices = FALSE,                        # Boolean. If TRUE, return consensus matrices for all considered values of K. Default is FALSE.
                              random_seed = NULL,                               # Set random seed for reproducibility. Default is NULL
                              parallel = TRUE                                   # Use parallel processsing. Default is TRUE.
                              ) 
  {   # Install relevant packages
      library(parallel)
  
      # Set random seed 
      if(!is.null(random_seed)){
        set.seed(random_seed)
      }
      
      # Intialise output list
      output <- list()
      
      n <- dim(moc)[1]
      
      if (is.null(k)) {
        stop("Parameter 'k' is required. Please specify a value or range for 'k'.")
      }
      
      # Run COCA with parallel processing
      if (parallel == TRUE) {
        
          if (length(k) != 1 & choiceKmethod == "silhouette") {
            
            # Create a cluster with available cores
            cl <- makeCluster(min(length(k), detectCores() - 1))
            
            # Export necessary variables and functions to the cluster
            clusterExport(cl, ls(envir = environment()), envir = environment())
            
            # Run the parallelized operation using parLapply
            results <- parLapply(cl, k, function(i) {
              
              ### Step 1: Compute consensus matrix for each k
              cm <- coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
                                 dist = ccDistHC, maxIterKM = max.iter)
              

              
              ### Step 2. Use hierarchical clustering on the consensus matrix
              dist_mat <- stats::as.dist(1 - cm)
              hc <- stats::hclust(dist_mat, method = hclustMethod)
              labels <- stats::cutree(hc, i)
              
              # Return results per k
              return(list(k = i, consensusMatrix = cm, clusterLabels = labels))
            })
            
            stopCluster(cl)
            
            # Combine results for silhouette evaluation
            consensusArray <- array(NA, dim = c(nrow(moc), nrow(moc), length(k)))
            clLabels <- matrix(NA, nrow = length(k), ncol = nrow(moc))
            
            for (j in seq_along(k)) {
              consensusArray[, , j] <- results[[j]]$consensusMatrix
              clLabels[j, ] <- results[[j]]$clusterLabels
            }
            
            consensusMatrix <- consensusArray
            K <- coca::maximiseSilhouette(consensusMatrix, clLabels, max(k), savePNG,
                                    fileName, widestGap = widestGap, dunns = dunns,
                                    dunn2s = dunn2s)$K
            
        # return(K)
        
        } else if (length(k) != 1 & choiceKmethod == "AUC") {
           
          # Create a cluster with available cores
          cl <- makeCluster(min(length(k), detectCores() - 1))
          
          # Export necessary variables and functions to the cluster
          clusterExport(cl, ls(envir = environment()), envir = environment())
          
          # Run the parallelized operation using parLapply
          results <- parLapply(cl, k, function(i) {
        
             ### Step 1. Compute the consensus matrix ###
             cm <- coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
                                dist = ccDistHC, maxIterKM = max.iter)
             ### Step 2. Compute area under the curve ###
             auc <- computeAUC(cm)
             
             return(list(k = i, consensusMatrix = cm, auc = auc))
           })
          
          stopCluster(cl)
          
          # Collect AUC values and consensus matrices
          areaUnderTheCurve <- sapply(results, function(res) res$auc)
          consensusArray <- array(NA, dim = c(nrow(moc), nrow(moc), length(k)))
          
          for (j in seq_along(k)) {
            consensusArray[, , j] <- results[[j]]$consensusMatrix
          }
          
           # Step 3: Pick K using AUC
           K <- coca::chooseKusingAUC(areaUnderTheCurve, savePNG, fileName)$K
           consensusMatrix <- consensusArray
        #  return(K)
           
        } else if (length(k) != 1) {
            stop("Method to choose number of clusters has not been recognised.
               Please make sure that it is either `silhouette` or `AUC`.")
         } 
          else if (length(k) == 1) {
           consensusMatrix <- NULL
           K <- k
          }
      }
        
        # Run COCA without parallel processing
        else {
            if (length(k) != 1 & choiceKmethod == "silhouette") {
              consensusMatrix <- array(NA, c(n, n, max(k) - 1))
              clLabels <- array(NA, c(max(k) - 1, n))
              
              for (i in seq_len(max(k) - 1) + 1) {
                ### Step 1. Compute the consensus matrix
                consensusMatrix[, ,i - 1] <-
                  coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
                                          dist = ccDistHC, maxIterKM = max.iter)
                
                ### Step 2. Use hierarchical clustering on the consensus matrix
                distances <- stats::as.dist(1 - consensusMatrix[, , i - 1])
                hClustering <- stats::hclust(distances, method = hclustMethod)
                clLabels[i - 1, ] <- stats::cutree(hClustering, i)
              }
              
              K <- coca::maximiseSilhouette(consensusMatrix, clLabels, max(k), savePNG,
                                            fileName, widestGap = widestGap, dunns = dunns,
                                            dunn2s = dunn2s)$K
              # return(K)
              
            } else if (length(k) != 1 & choiceKmethod == "AUC") { # I think this needs to be removed and changed to dunn!
              consensusMatrix <- array(NA, c(n, n, max(k) - 1))
              areaUnderTheCurve <- rep(NA, max(k) - 1)
              
              for (i in seq_len(max(k) - 1) + 1) {
                ### Step 1. Compute the consensus matrix ###
                consensusMatrix[, ,i - 1] <-
                  coca::consensusCluster(moc, i, B = N, pItem, clMethod = ccClMethod,
                                         dist = ccDistHC, maxIterKM = max.iter)
                ### Step 2. Compute area under the curve ###
                areaUnderTheCurve[i - 1] <- computeAUC(consensusMatrix[, , i - 1])
              }
              
              K <- coca::chooseKusingAUC(areaUnderTheCurve, savePNG, fileName)$K
              #  return(K)
              #
            } else if (length(k) != 1) {
              stop("Method to choose number of clusters has not been recognised.
               Please make sure that it is either `silhouette` or `AUC`.")
            } else if (length(k) == 1) {
              consensusMatrix <- NULL
              K <- k
            }
          }
      
      if (verbose)
        print(paste("K =", k, sep = " "))
      
      ### Step 1. Compute the consensus matrix ###
      if (!is.null(consensusMatrix)) {
         output$consensusMatrix <- consensusMatrix[, , K - 1]
         output$consensusMatrix <- matrix(as.integer(output$consensusMatrix),
                                          nrow = nrow(output$consensusMatrix),
                                          ncol = ncol(output$consensusMatrix))
      } else {
         output$consensusMatrix <- coca::consensusCluster(moc, K, B = N, pItem,
                                                    clMethod = ccClMethod,
                                                    dist = ccDistHC,
                                                   maxIterKM = max.iter)
         output$consensusMatrix <- matrix(as.integer(output$consensusMatrix),
                                          nrow = nrow(output$consensusMatrix),
                                          ncol = ncol(output$consensusMatrix))
      }

      ### Step 2. Use hierarchical clustering on the consensus matrix ###
      d <- stats::as.dist(1 - output$consensusMatrix)
      hC <- stats::hclust(d, method = hclustMethod)
      output$clusterLabels <- stats::cutree(hC, K)

      output$K <- K

      if (returnAllMatrices)
         output$consensusMatrices <- consensusMatrix

      return(output)
      
}
```

```{r multicoca, echo = FALSE}
# multicoca.R
multicoca <- function(moc_list,                                         # List of Matrix of Clusters
                      k = 2:9,                                          # Number of clusters. Default is to loop through k = 1 to k = 9.
                      N = 1000,                                         # Number of iterations of Consensus Clustering step.
                      max.iter = 1000,                                  # Maximum number of iterations for k-means clustering
                      pItem = 0.8,                                      # Proportion of items sampled at each iteration. 
                      hclustMethod = "average",                         # Agglomeration method to be used by the hclust function to perform hierarchical clustering on the consensus matrix. Can be "single","complete", "average", etc. For more details please see ?stats::hclust.
                      choiceKmethod = "silhouette",                     # Method used to choose the number of clusters if K is NULL, can be either "AUC" (area under the curve, work in progress) or "silhouette". Default is "silhouette".
                      ccClMethod = "kmeans",                            # Clustering method to be used by the Consensus Clustering algorithm (CC). Can be either "kmeans" for k-means clustering or "hclust" for hiearchical clustering. Default is "kmeans".
                      ccDistHC = "euclidean",                           # Distance to be used by the hiearchical clustering algorithm inside CC. Can be "pearson" (for 1 - Pearson correlation), "spearman" (for 1- Spearman correlation), or any of the distances provided in stats::dist() (i.e. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"). Default is "euclidean".
                      savePNG = FALSE,                                  # Boolean. Save plots as PNG files. Default is FALSE.
                      fileName = "coca",                                # Boolean. If savePNG is TRUE, this is the string containing (the first part of) the name of the output files. Can be used to specify the folder path too. Default is "coca". The ".png" extension is automatically added to this string.
                      verbose = FALSE,                                  # Boolean. 
                      widestGap = FALSE,                                # Boolean. If TRUE, compute also widest gap index to choose best number of clusters. Default is FALSE.
                      dunns = FALSE,                                    # Boolean. If TRUE, compute also Dunn's index to choose best number of clusters. Default is FALSE.
                      dunn2s = FALSE,                                   # Boolean. If TRUE, compute also alternative Dunn's index to choose best number of clusters. Default is FALSE.
                      returnAllMatrices = FALSE,                        # Boolean. If TRUE, return consensus matrices for all considered values of K. Default is FALSE.
                      random_seed = NULL,                               # Set random seed for reproducibility. Default is NULL
                      parallel = FALSE                                   # Use parallel processing. Default is TRUE.
                      )  
{
  
  # Load necessary libraries
  library(coca)
  library(parallel)
  
  # Set random seed 
  if(!is.null(random_seed)){
    set.seed(random_seed)
  }
  
  # Create an empty list to store the results
  results <- list()
  
  # Run MultiCOCA with parallel processing
  if (parallel == TRUE) {
    
    # Create a cluster with available cores
    cl <- makeCluster(detectCores() - 1)
    
    # Export necessary variables and functions to the cluster
    clusterExport(cl, varlist = c("clusterofclusters"))
    
    # Run the parallelized operation using parLapply
    results <- parLapply(cl, 1:length(moc_list), function(i) {
        result <- clusterofclusters(moc_list[[i]], 
                                    k = k,
                                    N = N, 
                                    max.iter = max.iter, 
                                    pItem = pItem, 
                                    hclustMethod = hclustMethod, 
                                    choiceKmethod = choiceKmethod, 
                                    ccClMethod = ccClMethod, 
                                    ccDistHC = ccDistHC, 
                                    savePNG = savePNG, 
                                    fileName = paste0(fileName, "_Group", i), 
                                    verbose = verbose, 
                                    widestGap = widestGap, 
                                    dunns = dunns, 
                                    dunn2s = dunn2s, 
                                    returnAllMatrices = returnAllMatrices,
                                    random_seed = random_seed,
                                    parallel = TRUE)
        
        # Name the results for each group
        return(result)
    })
    
    # Stop the cluster after computation
    stopCluster(cl)
    
    # Name each group
    names(results) <- paste0("Group", 1:length(results))
    
  } 
  
  # Run MultiCOCA without parallel processing
  else {
    for (i in 1:length(moc_list)) {
      results[[i]] <- clusterofclusters(moc_list[[i]], 
                                        k = k, 
                                        N = N, 
                                        max.iter = max.iter, 
                                        pItem = pItem, 
                                        hclustMethod = hclustMethod, 
                                        choiceKmethod = choiceKmethod, 
                                        ccClMethod = ccClMethod, 
                                        ccDistHC = ccDistHC, 
                                        savePNG = savePNG, 
                                        fileName = paste0(fileName, "_Group", i), 
                                        verbose = verbose, 
                                        widestGap = widestGap, 
                                        dunns = dunns, 
                                        dunn2s = dunn2s, 
                                        returnAllMatrices = returnAllMatrices,
                                        random_seed = random_seed,
                                        parallel = FALSE)
      names(results)[[i]] <- paste0("Group", i)
    }
  }
  
  # Return the results list
  return(results)
}
```

```{r groupARI, echo = FALSE}
# groupARI.R
groupARI <- function(output,
                     true_clusters){
  group_names <- names(results)
  ari_values <- data.frame(Group = character(),
                           ARI = numeric(),
                           stringsAsFactors = FALSE
                           )
  for (group in group_names){
    ari_values <- rbind(ari_values, data.frame(Group = group,
                                               ARI = adjustedRandIndex(
                                                 true_clusters[[paste0(tolower(group),"_clusterid")]],
                                                 output[[group]]$clusterLabels)
    ))
  }
  
  return(ari_values)
}
```

```{r constructMOC, echo = FALSE}
# constructMOC.R
constructMOC <- function(data,                    # Input as data frame or list of data frames.
                         ID = NULL,               # ID column for participants
                         parallel = FALSE         # Use parallel processsing. Default is TRUE.
                         )              

  {
    # Load relevant libraries
    library(parallel)
  
    # # Construct empty similarity matrix 
    # sim_matrix <- matrix(0, nrow = ncol(classification), ncol = ncol(classification),
    #                    dimnames = list(colnames(classification), colnames(classification)))
    # 
    # # Set seed 
    # set.seed(seed)
    # 
    # if (parallel) {
    #   
    #   # Obtain number of cores 
    #   n_cores <- numCores()
    #   
    #   if (.Platform$OS.type == "windows") {
    #     cl2 <- makeCluster(n_cores)
    #     clusterExport(cl2, ls(envir = environment()), envir = environment())
    #     
    #     # Parallelised computation of similarity matrix
    #     sim_mat <- parLapply(cl2, 1:ncol(classification), function(i) {
    #       sapply(1:ncol(classification), function(j) {
    #         adjustedRandIndex(classification[[i]], classification[[j]])
    #       })
    #     })
    #     
    #     # Stop the parallel cluster after the work is done
    #     stopCluster(cl2)
    #     
    #   } else {
    #     sim_mat <- mclapply(1:ncol(classification), function(i) {
    #       sapply(1:ncol(classification), function(j) {
    #         adjustedRandIndex(classification[[i]], classification[[j]])
    #       })
    #     }, mc.cores = n_cores)
    #   }
    # }
    # 
    # else {
    #   sim_mat <- lapply(1:ncol(classification), function(i) {
    #     sapply(1:ncol(classification), function(j) {
    #       adjustedRandIndex(classification[[i]], classification[[j]])
    #     })
    #   })
    #   
    # }
    # 
    # # Combine the list into a matrix
    # sim_matrix <- do.call(cbind, sim_mat)
    # 
    # # Convert to dissimilarity matrix
    # dissim_matrix <- 1 - sim_matrix
    # dist_mat <- as.dist(dissim_matrix)
    
    #### INSERT IN HERE MDS TEST CODE ####

    # Generate MOC for a single dataset
    if (class(data) != "list") {
      data <- as.data.frame(sapply(data, as.factor))
      moc <- do.call(cbind, lapply(data, function(x) model.matrix(~ x - 1)))
      moc <- as.matrix(moc)
      colnames(moc) <- NULL
      names(moc) <- "MOC"  # Ensure proper naming for the single MOC
    }
    
    # Generate MOC for multiple datasets
    else {
      
      # Only keep individuals that are common across all datasets
      if (length(unique(sapply(data, nrow))) != 1) {
        if(!is.null(ID)){
          common_IDs <- Reduce(intersect, lapply(data, function(data) data$ID))
          data <- lapply(data, function(data) data[data$ID %in% common_ids, ])
        }
        else {
          stop("If datasets are not provided with an ID column, then they need
                to have all the same number of rows (and the elements must be in
                the same order in each dataset), otherwise it is impossible
                to match observations from different datasets into the same
                matrix of clusters.")
        }
      }
      
      # With parallel processing
      if (parallel) {
      
        # Create empty list for MOC
        moc <- list()
        
        # If using Windows
        if (.Platform$OS.type == "windows"){
        
          # Create a cluster with available cores
          cl <- makeCluster(n_cores)
          
          # Export necessary variables and functions
          clusterExport(cl, c("data"))
          
          # Create MOC for each group of data
          moc <- parLapply(cl, 1:length(data), function(i) {
            data[[i]] <- as.data.frame(sapply(data[[i]], as.factor))
            m <- do.call(cbind, lapply(data[[i]], function(x) model.matrix(~ x - 1)))
            m <- as.matrix(m)
            colnames(m) <- NULL
            return(m)
          })
          
          # End the cluster
          stopCluster(cl)
        
        } else {
          moc <- mclapply(1:length(data), function(i) {
            data[[i]] <- as.data.frame(sapply(data[[i]], as.factor))
            m <- do.call(cbind, lapply(data[[i]], function(x) model.matrix(~ x - 1)))
            m <- as.matrix(m)
            colnames(m) <- NULL
            return(m)
          }, mc.cores = n_cores)
        } 
        names(moc) <- paste0("MOC - Group", 1:length(moc))
      }
    
      # Without parallel processing
      else  {
        moc <- list()
        for (i in 1:length(data)) {
          data[[i]] <- as.data.frame(sapply(data[[i]], as.factor))
          moc[[i]] <- do.call(cbind, lapply(data[[i]], function(x) model.matrix(~ x - 1)))
          moc[[i]] <- as.matrix(moc[[i]])
          colnames(moc[[i]]) <- NULL
          names(moc)[[i]] <- paste0("MOC - Group", i)  
        }
      }
    }
    return(moc)
  }
```

## Data Simulation

### Define Parameters

```{r params} 
N_col <- 30
n_groups <- 3
params <- list(
  cluster1 = list(mean = rnorm(N_col, mean = -3, sd = 0.1), cov = cov(matrix(rnorm(N_col*N_col, mean = 0.3, sd = 0.3), nrow = N_col, ncol = N_col))),
  cluster2 = list(mean = rnorm(N_col, mean = 0, sd = 0.2), cov = cov(matrix(rnorm(N_col*N_col, mean = 0.6, sd = 0.2), nrow = N_col, ncol = N_col))),
  cluster3 = list(mean = rnorm(N_col, mean = 3, sd = 0.2), cov = cov(matrix(rnorm(N_col*N_col, mean = -0.4, sd = 0.2), nrow = N_col, ncol = N_col)))
)
```

### Simulate Data from a Gaussian Mixture Model 

```{r}
data <- simulateGMM(3, n_groups, params, n_indiv = 419, n_col = N_col,
                     random_seed = seed,
                     equal_clust = FALSE, equal_groups = FALSE)
true_clusters <- data[[2]]
if (n_groups > 1) { 
  true_groups <- data[[3]]
}
data <- data[[1]]
```
### Plot of Simulated Data

```{r, echo = FALSE}
data_plot <- as.matrix(data)
annotationRow <- as.data.frame(true_clusters[1])
names(annotationRow) <- "Clusters"

annotationCol <- as.data.frame(as.factor(true_groups))
names(annotationCol) <- "ProteinClusters"
rownames(annotationCol) <- colnames(data_plot)

rownames(data_plot) <- rownames(annotationRow)
annotationRow$Clusters <- as.factor(annotationRow$Clusters)
annotationCol$ProteinClusters <- as.factor(annotationCol$ProteinCluster)
true_clust <- as.numeric(true_clusters[["group1_clusterid"]])
ordered_data <- data_plot[order(true_clust), order(annotationCol$ProteinClusters)]
pheatmap::pheatmap(
  ordered_data,
  annotation_row = annotationRow,
  annotation_col = annotationCol[order(annotationCol$ProteinClusters), , drop = FALSE],
  cluster_rows = FALSE
)
```

## Classification

```{r}
# Classify data using a GMM 
classification <- GMMclassifier(data)
```

## Hierarchical Clustering of Groups

```{r, echo = FALSE}
# Construct similarity matrix 
sim_matrix <- matrix(0, nrow = ncol(classification), ncol = ncol(classification),
                     dimnames = list(colnames(classification), colnames(classification)))

set.seed(seed)
for (i in 1:ncol(classification)){
  for(j in 1:ncol(classification)){
    sim_matrix[i,j] <- adjustedRandIndex(classification[[i]], classification[[j]]) # we have called this M (capital)
  }
}

# Convert to dissimilarity matrix
dissim_matrix <- 1 - sim_matrix
dist_mat <- as.dist(dissim_matrix)
```

### Heatmap of the Similarity Matrix

```{r}
# Pheatmap
pheatmap::pheatmap(sim_matrix)
```

### Testing different linkages and indices 

```{r} 
# Specify methods and indices to test
methods <- list("single","complete","average","ward.D2","ward.D","mcquitty","median","centroid")
indices <- list("cindex","silhouette","dunn","mcclain")
```

```{r, include=FALSE}
opt_hclust <- optimal_hclust(dist_mat,
                             n_groups,
                             method = methods,
                             index = indices)
```
```{r, echo = FALSE}
# Choose best index and linkage function
opt_hclust$`Number of Clusters`

# Plot results 
indices <- opt_hclust$`Index Values`
ggplot(data = indices, aes(x = NClust, y = Value)) + 
  geom_line() + 
  facet_grid(Method ~ Index) +  # Facet by both Method and Index
  labs(
    title = "Clustering Indices by Number of Clusters",
    x = "Number of Clusters",
    y = "Index Value"
  ) +
  theme_minimal()
```

```{r}
# Apply based on the results above
hclust <- hclust(dist_mat, method = "complete")
opt <- NbClust(diss = dist_mat, distance = NULL, method = "single", 
               min.nc = 2, max.nc = 9, index = "dunn")
```

```{r, echo = FALSE}
# Choose optimal 
if (is.vector(opt$Best.nc)) {
  opt_ng <- as.numeric(opt$Best.nc[1])
} else {
  opt_ng <- as.numeric(names(which.max(table(opt$Best.nc[1, ]))))
}
```

```{r}
print(paste("Optimal Number of Groups:", opt_ng))
```

```{r}
# Split dendogram
groups <- cutree(hclust, k = opt_ng)
```

```{r, echo = FALSE}
# Test grouping 
if (exists("true_groups")) {
  print(paste("ARI for the derived groups vs the true groups:", adjustedRandIndex(groups, true_groups)))
}
```

```{r}
# Split the columns into datasets by group 
data_group <- list()

for(i in 1:length(unique(groups))){
  data_group[[i]] <- classification[groups == i]
  names(data_group)[[i]] <- paste0("Group",i)
}

# Create a MOC for each group
moc <- constructMOC(data_group)

# Try MultiCOCA
results <- multicoca(moc, ccClMethod = "hclust", hclustMethod = "complete", 
                        random_seed = 4881, N = 1000, max.iter = 1000, 
                        parallel = TRUE)
```

```{r, echo = FALSE}
# Test against true values
ARI_group <- groupARI(results, true_clusters)
print("ARI for the derived clustering by group vs true clustering by group")
print(ARI_group)
```
